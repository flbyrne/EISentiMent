{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import urllib.request\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import datetime, timedelta,date\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import dill\n",
    "from ediblepickle import checkpoint\n",
    "from retrying import retry\n",
    "import os\n",
    "\n",
    "import searchtweets\n",
    "from searchtweets import ResultStream, gen_rule_payload, load_credentials\n",
    "from updates import get_response, get_EOD_data, get_st_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Get historical unemployment rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_UR=pd.read_html('https://data.bls.gov/timeseries/LNU04000000')[1]\n",
    "df_UR.rename(columns={'Unnamed: 0':'Year'},inplace=True)\n",
    "df_UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(df_UR, open('data/df_UR.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_UR = dill.load(open('data/df_UR.pkd', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Organize historical unemployment rates into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates=pd.Series()\n",
    "years=pd.Series()\n",
    "for row in df_UR.index:\n",
    "    rates=rates.append(df_UR.iloc[row]['Jan':])\n",
    "    years=years.append(pd.Series([int(df_UR.iloc[row]['Year'])]*12))\n",
    "changes=[round((rates[i]-rates[i-1])/rates[i-1]*100,2) for i in range(1,len(rates))]\n",
    "changes.insert(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp=pd.DataFrame(rates).reset_index().rename(columns={'index':'Month',0:'Revised Rate'})\n",
    "df_temp['Year']=list(years)\n",
    "df_UR_Releases=pd.DataFrame()\n",
    "df_UR_Releases['Period']=df_temp['Month']+df_temp['Year'].apply(str)\n",
    "df_UR_Releases['Period']=df_UR_Releases['Period'].apply(lambda x:x[:3]+' '+x[3:])\n",
    "df_UR_Releases['Revised Rate']=df_temp['Revised Rate']\n",
    "df_UR_Releases['Percent Change']=list(changes)\n",
    "df_UR_Releases=df_UR_Releases.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Add unemployment releases to data frame\n",
    "\n",
    "###### 3.1 Retrieve links to text for each release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkpage=BeautifulSoup(urllib.request.urlopen('https://www.bls.gov/bls/news-release/empsit.htm').read(), 'lxml')\n",
    "linkpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items=linkpage.find_all('a')\n",
    "list_UR_links=[]\n",
    "for item in items:\n",
    "    if 'Employment Situation' in item.text and '/news.release/archives/empsit_' in item['href']:\n",
    "        url='https://www.bls.gov'+re.search('(/news.*)',item['href']).group(0)\n",
    "        list_UR_links.append(url)\n",
    "list_UR_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.2 Extract relevant text from each release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "announcements=[]\n",
    "dates=[]\n",
    "for link in list_UR_links:\n",
    "    d=datetime.strptime(re.search('\\/empsit_(\\d{8})\\.',link).group(1),'%m%d%Y')\n",
    "    if d>=datetime(2010,2,1):\n",
    "        announcement=BeautifulSoup(requests.get(link).text,'lxml').find('pre')\n",
    "        announcement=re.search('rate[,\\s]?[\\w\\s\\d\\.]+ percent',announcement.text.replace('\\n',' ')).group(0)\n",
    "        announcements.append(announcement)\n",
    "        dates.append(d)\n",
    "announcements.reverse()\n",
    "dates.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_UR_Releases['Release Date']=dates\n",
    "df_UR_Releases['Announcement']=announcements\n",
    "df_UR_Releases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.2.1 Assign sentiment to each release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_words=[]\n",
    "for item in df_UR_Releases['Announcement']:\n",
    "    sentiment_words.append(re.search('[A-Za-z\\s]+',item.replace('rate','').replace('percent','')).group(0))\n",
    "set(sentiment_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def release_tone(txt):\n",
    "    \"\"\"Assigns a -1 when comments are positive\"\"\"\n",
    "    \"\"\"Remember: Unemployment rates going DOWN IS GOOD\"\"\"\n",
    "    positive=['edged up','rose']\n",
    "    negative=['declined','decreased','edged down','fell']\n",
    "    for word in positive:\n",
    "        if word in txt:\n",
    "            return -1\n",
    "    for word in negative:\n",
    "        if word in txt:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3.2.2 Calculate unemployment rate changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_changes(alist):\n",
    "    return [round((alist[i]-alist[i-1])/alist[i-1]*100,2) for i in range(1,len(alist))]\n",
    "df_UR_Releases['Tone']=df_UR_Releases['Announcement'].apply(release_tone)\n",
    "df_UR_Releases['Announced Value']=df_UR_Releases['Announcement'].str.extract(r'(\\d+\\.\\d)( percent$)').loc[:,0].apply(float)\n",
    "df_UR_Releases['Announced Percent Change']=[-3]+calc_changes(list(df_UR_Releases['Announced Value']))\n",
    "df_UR_Releases['Discrepancy']=df_UR_Releases['Announced Value']-df_UR_Releases['Revised Rate']\n",
    "df_UR_Releases=df_UR_Releases.set_index('Period')\n",
    "df_UR_Releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(df_UR_Releases, open('data/df_UR_Releases.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Get list of tickers for stock sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SP500=pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0]\n",
    "df_SP500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(df_SP500, open('data/df_SP500.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers=list(df_SP500['Symbol'])\n",
    "tickers[0]='SPY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(tickers, open('data/tickers.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Retrieve daily historical data for stocks (https://www.tiingo.com API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = dill.load(open('data/tickers.pkd', 'rb'))\n",
    "d_all_EOD={}\n",
    "errors=[]\n",
    "for n,ticker in enumerate(tickers):\n",
    "    try:\n",
    "        df_EOD=pd.DataFrame(get_EOD_data(ticker,'2010-1-1','2020-10-17').json())\n",
    "        df_EOD['date']=df_EOD['date'].apply(lambda x:x[:10]).apply(lambda x:datetime.strptime(x,'%Y-%m-%d'))\n",
    "        d_all_EOD[ticker]=df_EOD\n",
    "    except Exception as err:\n",
    "        errors.append((ticker,err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(d_all_EOD, open('data/d_all_EOD.pkd', 'wb'))\n",
    "dill.dump(errors, open('data/errors.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_all_EOD = dill.load(open('data/d_all_EOD.pkd', 'rb'))\n",
    "errors = dill.load(open('data/errors.pkd', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tickers=list(d_all_EOD.keys())\n",
    "valid_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(valid_tickers, open('data/valid_tickers.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tickers = dill.load(open('data/valid_tickers.pkd', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Retrieve data from stocktwits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = 'data/STcache1'\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.mkdir(cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = dill.load(open('data/tickers.pkd', 'rb'))\n",
    "d_ST_messages={}\n",
    "for ticker in tickers:\n",
    "    url='https://api.stocktwits.com/api/2/streams/symbol/'+ticker+'.json'\n",
    "    try:\n",
    "        d_ST_messages[ticker]=get_st_messages(url,ticker)\n",
    "    except NameError:\n",
    "        print('URL not found for', ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ST_messages_alldat[datetime(datetime.now().year,datetime.now().month,datetime.now().day)]=d_ST_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(d_ST_messages_alldat, open('data/d_ST_messages_alldat.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Retrieve data from twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_to_string(date,form):\n",
    "    str_year=str(date.year)\n",
    "    str_month=str(date.month)\n",
    "    str_day=str(date.day)\n",
    "    if date.month<10:\n",
    "        str_month='0'+str_month\n",
    "    if date.day<10:\n",
    "        str_day='0'+str_day\n",
    "    if form==0:\n",
    "        return str_year+str_month+str_day+'0000'\n",
    "    elif form==1:\n",
    "        return str_year+'-'+str_month+'-'+str_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(ticker_list,date):\n",
    "    premium_search_args = load_credentials(filename=\"twitter_keysG.yml\",\n",
    "                                       yaml_key=\"search_tweets_fullarchive_dev\",env_overwrite=False)\n",
    "    rule_str=' OR '.join(ticker_list)+' lang:en'\n",
    "    date1=convert_date_to_string(date-timedelta(days=1),0)\n",
    "    date2=convert_date_to_string(date,0)\n",
    "    rule = gen_rule_payload(pt_rule=rule_str,from_date=date1,to_date=date2)\n",
    "    rs = ResultStream(rule_payload=rule,**premium_search_args)\n",
    "    return list(rs.stream())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_index=130\n",
    "second_index=140\n",
    "while second_index<len(tickers):\n",
    "    print(tickers[first_index:second_index])\n",
    "    date=list(df_UR_Releases['Release Date'])[-2]\n",
    "    tweet_list=get_tweets(tickers[first_index:second_index],date)\n",
    "    for ticker in tickers[first_index:second_index]:\n",
    "        tweets[date][ticker]=[tweet['text'] for tweet in tweet_list if '$'+ticker in tweet['text']]\n",
    "    first_index+=10\n",
    "    second_index+=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_UR_Releases = dill.load(open('data/df_UR_Releases.pkd', 'rb'))\n",
    "date=list(df_UR_Releases['Release Date'])[-2]\n",
    "tweets[date]={}\n",
    "tweet_list=get_tweets(tickers[:10],date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(tweets, open('data/tweets.pkd', 'wb'))\n",
    "dill.dump(tweet_list, open('data/tweet_list.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Get news from Benzinga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = 'data/BenzNewscache'\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.mkdir(cache_dir)\n",
    "\n",
    "@checkpoint(key=lambda args,kwargs:args[0], work_dir=cache_dir)\n",
    "def get_news(ticker):\n",
    "    url='https://www.benzinga.com/stock-articles/'+ticker+'/news'\n",
    "    all_headlines={}\n",
    "    next_page=''\n",
    "    while next_page!=None:\n",
    "        try:\n",
    "            print(url)\n",
    "            newspage=BeautifulSoup(get_response(url).text,'lxml')\n",
    "        except NameError:\n",
    "            return None\n",
    "        news_list=newspage.find_all('div', attrs={'class':'item-list'})\n",
    "        for item in news_list:\n",
    "            if item.find('h3')!=None:\n",
    "                date=datetime.strptime(item.find('h3').text,'%A, %B %d, %Y')\n",
    "                headlines=item.find_all('span', attrs={'class':'field-content'})\n",
    "                for index,headline in enumerate(headlines):\n",
    "                    headlines[index]=headline.find('a').text\n",
    "                all_headlines[date]=headlines\n",
    "                print(date,headlines)\n",
    "        next_page=newspage.find('a', attrs={'title':\"Go to next page\"})\n",
    "        if next_page==None:\n",
    "            break\n",
    "        else:\n",
    "            url='https://www.benzinga.com'+next_page['href']\n",
    "    return all_headlines\n",
    "get_news('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = dill.load(open('data/tickers.pkd', 'rb'))\n",
    "for ticker in tickers:\n",
    "    all_ticker_news[ticker]=get_news(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(all_ticker_news, open('data/all_ticker_news.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Get unemployment predictions from Trading Economics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_UR_prediction():\n",
    "    date=datetime.now()\n",
    "    table=pd.read_html('https://tradingeconomics.com/united-states/unemployment-rate')[1]\n",
    "    dates=list(table['Calendar'].apply(lambda x:datetime.strptime(x,'%Y-%m-%d')))\n",
    "    table.set_index('Calendar', inplace=True)\n",
    "    for d in dates:\n",
    "        if d>=date:\n",
    "            next_release_date=datetime.strftime(d+timedelta(hours=8.5),'%Y-%m-%d')\n",
    "            try:\n",
    "                predicted_rate=float(table.loc[datetime.strftime(d,'%Y-%m-%d'),'TEForecast'][:-1])\n",
    "            except:\n",
    "                predicted_rate=0\n",
    "            break\n",
    "    return next_release_date,predicted_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_UR_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_UR_Releases = dill.load(open('data/df_UR_Releases.pkd', 'rb'))\n",
    "def get_UR_current():\n",
    "    periods=df_UR_Releases.index\n",
    "    return df_UR_Releases.loc[periods[-1],'Revised Rate'],df_UR_Releases.loc[periods[-1],'Percent Change']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_UR_current()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Get sentiment indicators from Benzinga (Stock Snips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(ticker):\n",
    "    page=BeautifulSoup(get_response('https://www.benzinga.com/stock/'+ticker).text,'lxml')\n",
    "    try:\n",
    "        sentiment=re.search('\\d{1,2}\\.\\d{1,2}\\%',page.find('div',attrs={'class':\"stock-snips-content\"}).text).group(0)[:-1]\n",
    "    except AttributeError:\n",
    "        print(ticker)\n",
    "        return 0\n",
    "    return float(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dict= dill.load(open('data/sentiment_dict.pkd', 'rb'))\n",
    "tickers = dill.load(open('data/tickers.pkd', 'rb'))\n",
    "\n",
    "today=datetime(datetime.now().year,datetime.now().month,datetime.now().day)\n",
    "sent_today={}\n",
    "for ticker in tickers:\n",
    "    sent_today[ticker]=get_sentiment(ticker)\n",
    "sentiment_dict[today]=sent_today\n",
    "dill.dump(sentiment_dict, open('data/sentiment_dict.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(sentiment_dict, open('data/sentiment_dict.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
